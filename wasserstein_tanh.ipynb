{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Overview\n",
    "This notebook contains the source code corresponding to the paper for tanh-normalization.\n",
    "The example dataset here is a synthetic data set; but the model architecture can supports any other dataset.\n",
    "Set synthetic=False to use a real world dataset (specific information and license at: https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import copy\n",
    "import gc\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.optimize import minimize_scalar\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import wasserstein_distance\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import optim\n",
    "from tqdm.notebook import tqdm\n",
    "from ucimlrepo import fetch_ucirepo"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T14:22:34.175320Z",
     "end_time": "2023-04-05T14:22:40.830820Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyper-parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CONFIG = {\"dataset_name\": \"synthetic\",\n",
    "          \"model_name\": \"FeedForward\",\n",
    "          \"device\": torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu'),\n",
    "          \"lr\": 0.001,\n",
    "          \"weight_decay\": 1e-4,\n",
    "          \"batch_size\": 256,\n",
    "          \"pin_memory\": True,\n",
    "          \"epochs\": 150,\n",
    "          \"n_times\": 10,\n",
    "          \"verbose_rate\": 5,\n",
    "          \"n_workers\": 8,\n",
    "          \"precomputed\": False,\n",
    "          \"n_classes\": 10\n",
    "          }\n",
    "print(CONFIG['device'], 'Device count:', torch.cuda.device_count())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset\n",
    "This code defines a custom (synthetic) dataset class and a function to generate training and test data.\n",
    "The CustomDataset class takes in data, labels, means and std as inputs.\n",
    "The data is first normalized using the given means and standard deviation, and stored as a tensor.\n",
    "The labels are stored as-is.\n",
    "The len method returns the length of the dataset.\n",
    "The getitem method returns the data and label at the given index.\n",
    "The get_dataset function generates a synthetic classification dataset using scikit-learn's make_classification function.\n",
    "It splits the dataset into training and test sets, and calculates the means and standard deviation of the training data.\n",
    "The function returns the training data, test data, training labels, test labels, and the means and standard deviation of the training data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    # Initialize the dataset with data, labels, means, and standard deviations\n",
    "    def __init__(self, data, labels, means, std):\n",
    "        # Store the data normalized by mean and standard deviation\n",
    "        self.data = torch.tensor((data - means) / std).float()\n",
    "        # Store the labels\n",
    "        self.labels = labels\n",
    "\n",
    "    # Define the length of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # Define how to get an item from the dataset\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# Define a function to create the dataset and split it into training and testing sets\n",
    "def get_dataset(synthetic=True):\n",
    "    if synthetic:\n",
    "        # Generate synthetic data with the specified number of classes, informative features, and total features\n",
    "        X, y = make_classification(n_samples=10000, n_classes=CONFIG[\"n_classes\"], n_informative=10, n_features=12)\n",
    "    else:\n",
    "        # fetch dataset \n",
    "        cdc_diabetes_health_indicators = fetch_ucirepo(id=891)\n",
    "        # data (as pandas dataframes) \n",
    "        X = cdc_diabetes_health_indicators.data.features\n",
    "        y = cdc_diabetes_health_indicators.data.targets.astype(np.int32)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    # Calculate the mean and standard deviation of the training data\n",
    "    mean, std = X_train.mean(), X_train.std()\n",
    "    # Return the training and testing data along with their labels and mean and standard deviation\n",
    "    return X_train, X_test, y_train, y_test, mean, std"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimal Tanh-Normalization\n",
    "This cell defines a function called calculate_optimal_values(), which calculates the optimal alpha values for a given dataset.\n",
    "First, the training data is converted into a NumPy array. Then, a Gaussian distribution is created with a specified quantile value. This Gaussian distribution is used to optimize the alpha values for each feature in the dataset. The func(x, idx) function takes in an alpha value and an index for a feature and returns the Wasserstein distance between the distribution of the hyperbolic tangent of the alpha multiplied by that feature and the Gaussian distribution created earlier. The minimize_scalar function from scipy is used to find the optimal alpha value for each feature. The bounds parameter sets the lower and upper bounds for the optimization, and the args parameter passes the index of the feature being optimized to the func() function. The method parameter specifies the optimization method. The alphas list is filled with the optimal alpha value for each feature. These values are then converted to a PyTorch tensor and printed. Finally, the alphas tensor is returned from the function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a function to calculate optimal alpha values\n",
    "def calculate_optimal_values():\n",
    "    # Convert training data to numpy array\n",
    "    data_std = train_set.data.numpy()\n",
    "\n",
    "    # Define a quantile value and create a Gaussian distribution\n",
    "    q = 0.001\n",
    "    x = np.linspace(norm.ppf(q), norm.ppf(1 - q), int(1 / q))\n",
    "    y = norm.pdf(x) * len(x)\n",
    "    z = torch.tensor([item for sublist in [[i] * int(j) for i, j in zip(x, y)] for item in sublist])\n",
    "    gaussian = 2 * (z - z.min()) / (z.max() - z.min()) - 1\n",
    "\n",
    "    # Define a function to optimize alpha values\n",
    "    def func(x, idx):\n",
    "        # Apply hyperbolic tangent to data\n",
    "        t = np.tanh(x * data_std[:, idx])\n",
    "        # Calculate Wasserstein distance between distribution of t and Gaussian distribution\n",
    "        wd = wasserstein_distance(t, gaussian)\n",
    "        return wd\n",
    "\n",
    "    # Optimize alpha values for each feature in the data\n",
    "    alphas = []\n",
    "    for i in range(data_std.shape[1]):\n",
    "        # Use scipy's minimize_scalar function to find the optimal alpha value for each feature\n",
    "        alphas.append(minimize_scalar(func, bounds=(0, 1), method='bounded', args=i)[\"x\"])\n",
    "        # Print a progress indicator for each feature\n",
    "        print(\">\", end=\"\")\n",
    "    print(\".\")\n",
    "    # Convert alpha values to a tensor and print them\n",
    "    alphas = torch.tensor(alphas)\n",
    "    print(\"Alphas:\", alphas)\n",
    "\n",
    "    # Return alpha values\n",
    "    return alphas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Normalization & training set-up"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, mean, std = get_dataset()\n",
    "train_set = CustomDataset(X_train, y_train, mean, std)\n",
    "test_set = CustomDataset(X_test, y_test, mean, std)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "alphas = calculate_optimal_values()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(train_set.data.numpy()).plot.kde()\n",
    "plt.title(\"Standardized\")\n",
    "pd.DataFrame(np.tanh(train_set.data.numpy() * 0.01)).plot.kde()\n",
    "plt.title(\"Tanh alpha=0.01\")\n",
    "pd.DataFrame(np.tanh(train_set.data.numpy() * alphas.numpy())).plot.kde()\n",
    "plt.title(\"Tanh alpha=optimal\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a neural network model\n",
    "class MainModel(nn.Module):\n",
    "    def __init__(self, in_features, n_classes, layer_size, layer_count, activation_function):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define a helper function to return a set of layers to be repeated\n",
    "        def _get_items():\n",
    "            return nn.Linear(layer_size, layer_size), nn.LayerNorm(layer_size), activation_function()\n",
    "\n",
    "        # Define the input layer and output layer, and create a sequence of layers with layer_count repetitions\n",
    "        self.in_layer = nn.Sequential(nn.Linear(in_features, layer_size), activation_function())\n",
    "        self.seq = nn.Sequential(*[x for _ in range(layer_count) for x in _get_items()])\n",
    "        self.out_layer = nn.Linear(layer_size, n_classes)\n",
    "\n",
    "    # Define the forward pass of the neural network\n",
    "    def forward(self, x):\n",
    "        return self.out_layer(self.seq(self.in_layer(x)))\n",
    "\n",
    "\n",
    "# Define a variable hyperbolic tangent activation function with variable alpha values\n",
    "class VariableTanh(nn.Module):\n",
    "    def __init__(self, alphas, random_init=False, fixed=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize alpha values with random values if specified\n",
    "        if random_init:\n",
    "            alphas = torch.rand_like(alphas)\n",
    "\n",
    "        # Calculate the inverse sigmoid of alpha values and reshape to be a row vector\n",
    "        inv_sig = torch.log(alphas / (1 - alphas))\n",
    "        self.a = inv_sig.reshape(1, -1).to(CONFIG['device']).float()\n",
    "\n",
    "        # Convert alpha values to trainable parameters if not fixed\n",
    "        if not fixed:\n",
    "            self.a = nn.Parameter(self.a)\n",
    "\n",
    "        # Initialize alpha history\n",
    "        self.alpha_history = []\n",
    "\n",
    "    # Calculate alpha values\n",
    "    @property\n",
    "    def alphas(self):\n",
    "        return torch.sigmoid(self.a)\n",
    "\n",
    "    # Record alpha values for each epoch\n",
    "    def record_alpha(self, epoch):\n",
    "        self.alpha_history.append([epoch] + self.alphas.detach().flatten().tolist())\n",
    "\n",
    "    # Define the forward pass of the activation function\n",
    "    def forward(self, x):\n",
    "        return torch.tanh(self.alphas * x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training logic"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a function for running analysis, which takes in three parameters: path, instances, and alphas.\n",
    "def run_analysis(path, instances, alphas):\n",
    "    # Get the number of classes from the global variable CONFIG dictionary.\n",
    "    n_classes = CONFIG['n_classes']\n",
    "    # Print the number of classes to the console.\n",
    "    print(f\"[num_classes={n_classes}]\")\n",
    "    # Loop over the number of times specified in the global variable CONFIG dictionary.\n",
    "    for run_idx in tqdm(range(CONFIG['n_times'])):\n",
    "        # Calculate the number of features by flattening the array of alphas and getting its length.\n",
    "        in_features = len(alphas.flatten())\n",
    "        # Create a new instance of the MainModel class with the specified configuration settings.\n",
    "        original_model = MainModel(in_features=in_features, n_classes=n_classes, layer_size=64, layer_count=4,\n",
    "                                   activation_function=nn.SiLU)\n",
    "        # Check if this is the first run. If it is, include the header when writing to the output CSV file.\n",
    "        include_header = run_idx == 0\n",
    "        # Loop over the instances (datasets) in reverse order.\n",
    "        for name, train_loader, test_loader, transform in reversed(instances):\n",
    "            # Create dictionaries to store the training and test loss values for this instance.\n",
    "            train_loss_dict, test_loss_dict = {}, {}\n",
    "            # We want the same weight initialization on all comparisons, so make a deep copy of the original model.\n",
    "            model = copy.deepcopy(original_model).to(CONFIG[\"device\"])\n",
    "            # Create a new instance of the VariableTanh class with the specified transformation settings.\n",
    "            var_tanh = VariableTanh(**transform)\n",
    "            # Combine the VariableTanh layer with the model using a Sequential container.\n",
    "            model = nn.Sequential(var_tanh, model)\n",
    "            # Create dictionaries to store the training and test loss values for this instance.\n",
    "            train_loss_dict[name] = pd.DataFrame()\n",
    "            test_loss_dict[name] = pd.DataFrame()\n",
    "            # Create a new instance of the AdamW optimizer with the specified learning rate and weight decay.\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\n",
    "            # Create a new instance of the CosineAnnealingLR learning rate scheduler with the specified maximum number of epochs.\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epochs'])\n",
    "            # Create a new instance of the CrossEntropyLoss loss function.\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            # Print the type of instance (dataset) to the console.\n",
    "            print(\"=\" * 40)\n",
    "            print('Type: ' + name)\n",
    "            # Initialize lists to store the training and test loss values, learning rates, and best scores seen so far.\n",
    "            test_loss = []\n",
    "            train_loss = []\n",
    "            lrs = []\n",
    "            best_train, best_test = 0, 0\n",
    "            # Loop over the specified number of epochs.\n",
    "            for epoch in tqdm(range(CONFIG['epochs'])):\n",
    "                # Record the alpha values at each epoch, if the model has a 'record_alpha' attribute.\n",
    "                if hasattr(model, 'record_alpha'):\n",
    "                    model.record_alpha(epoch)\n",
    "                # Get the current learning rate from the scheduler.\n",
    "                _lr = scheduler.get_last_lr()[0]\n",
    "                lrs.append(_lr)\n",
    "                # Record the starting time of this epoch.\n",
    "                start = time.time()\n",
    "                # Initialize a variable to store the running loss.\n",
    "                running_loss = 0.0\n",
    "                # Initialize a variable to keep track of the batch index.\n",
    "                k = -1\n",
    "                for i, _data in enumerate(train_loader):\n",
    "                    k += 1  # Increment the batch index counter.\n",
    "                    inputs, labels = _data\n",
    "                    inputs = inputs.to(CONFIG['device'])\n",
    "                    labels = labels.to(CONFIG['device'], non_blocking=True)\n",
    "                    # Learn\n",
    "                    optimizer.zero_grad()  # Reset gradients to zero.\n",
    "                    outputs = model.forward(inputs)  # Perform forward pass.\n",
    "                    loss = criterion(outputs, labels)  # Calculate loss.\n",
    "                    loss.backward()  # Perform backward pass and compute gradients.\n",
    "                    optimizer.step()  # Update weights using calculated gradients.\n",
    "                    # Store statistics.\n",
    "                    running_loss += loss.item()  # Add the batch loss to the running loss.\n",
    "                scheduler.step()  # Update the learning rate scheduler.\n",
    "                # Get scores\n",
    "                if epoch % CONFIG['verbose_rate'] == 0 or epoch == CONFIG['epochs'] - 1:\n",
    "                    with torch.no_grad():\n",
    "                        model.eval()  # Set the model to evaluation mode.\n",
    "                        for stuff in [(train_loss, train_loader), (test_loss, test_loader)]:\n",
    "                            record, loader = stuff\n",
    "                            labels_all = []\n",
    "                            predicted_all = []\n",
    "                            for _data in loader:\n",
    "                                inputs, labels = _data\n",
    "                                inputs = inputs.to(CONFIG['device'])\n",
    "                                labels = labels.to(CONFIG['device'], non_blocking=True)\n",
    "                                if name == \"Trainable WD-Tanh\":\n",
    "                                    # Apply tanh activation function (if required).\n",
    "                                    inputs = model.apply_tanh(inputs)\n",
    "                                # Perform forward pass.\n",
    "                                outputs = model.forward(inputs)\n",
    "                                # Get the predicted classes.\n",
    "                                _, predicted = torch.max(outputs.detach(), 1)\n",
    "                                # Append the true classes to the labels list.\n",
    "                                labels_all.extend(labels.squeeze().tolist())\n",
    "                                # Append the predicted classes to the predicted list.\n",
    "                                predicted_all.extend(predicted.squeeze().tolist())\n",
    "                                # Calculate the accuracy score.\n",
    "                            score = accuracy_score(labels_all, predicted_all)\n",
    "                            # Append the score to the corresponding record.\n",
    "                            record.append(score)\n",
    "                        # Set the model back to training mode.\n",
    "                        model.train()\n",
    "                    # Calculate the time taken for the epoch.\n",
    "                    minutes, seconds = divmod(time.time() - start, 60)\n",
    "                    # Update the best training loss so far.\n",
    "                    best_train = max(best_train, train_loss[-1])\n",
    "                    # Update the best testing loss so far.\n",
    "                    best_test = max(best_test, test_loss[-1])\n",
    "                    # Print the statistics for the epoch.\n",
    "                    print(('[%d | %d] Lr: %.3f \\tLoss: %.3f \\tTrain score: %.2f |\\tTest score: %.2f \\t(%d min. %d s.)' %\n",
    "                           (run_idx, epoch, _lr, running_loss, train_loss[-1], test_loss[-1], minutes, seconds)))\n",
    "            # Print final alpha value for the Tanh function if its name matches.\n",
    "            if name == \"Tanh [trainable]\":\n",
    "                print(f\"Final alpha={var_tanh.alphas.flatten()}\")\n",
    "\n",
    "            # Clear GPU memory cache if GPU is used.\n",
    "            if CONFIG['device'] != \"cpu\":\n",
    "                with torch.cuda.device(CONFIG['device']):\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            # Print best train and test losses for the current method.\n",
    "            print(f\"{name}:\\tBest: train_loss={round(best_train, 3)} & test_loss={round(best_test, 3)}\")\n",
    "\n",
    "            # Assuming train_loss_dict[name] and test_loss_dict[name] are DataFrame objects\n",
    "            train_loss_dict[name] = pd.concat([train_loss_dict[name], pd.DataFrame(list(enumerate(train_loss)))])\n",
    "            test_loss_dict[name] = pd.concat([test_loss_dict[name], pd.DataFrame(list(enumerate(test_loss)))])\n",
    "\n",
    "            # Update the columns of the train and test loss dataframes with 'Epoch' and 'Score' and add 'Type' and 'Method' columns.\n",
    "            for t, d in [('Test', test_loss_dict), ('Train', train_loss_dict)]:\n",
    "                d[name].columns = [\"Epoch\", \"Score\"]\n",
    "                d[name][\"Type\"] = t\n",
    "                d[name][\"Method\"] = name\n",
    "                d[name][\"Epoch\"] *= CONFIG['verbose_rate']\n",
    "\n",
    "            # Concatenate the train and test loss dataframes and write to a CSV file.\n",
    "            d = pd.concat(list(test_loss_dict.values()) + list(train_loss_dict.values()))\n",
    "            d.to_csv(path, index=False, mode='w' if include_header else 'a', header=include_header)\n",
    "\n",
    "            # If the model has an 'alpha_history' attribute, write its value to a CSV file.\n",
    "            if hasattr(model, 'alpha_history'):\n",
    "                pd.DataFrame(model.alpha_history).to_csv(\"alpha_history_\" + path, index=False,\n",
    "                                                         mode='w' if include_header else 'a', header=include_header)\n",
    "\n",
    "            # Set include_header flag to False after the first iteration of writing to the CSV file.\n",
    "            include_header = False\n",
    "\n",
    "            # Free up memory by deleting the optimizer and model objects and running the garbage collector.\n",
    "            del optimizer\n",
    "            del model\n",
    "            gc.collect()\n",
    "\n",
    "            # Print a message indicating that the information has been stored to the CSV file.\n",
    "            print(\"Stored to csv.!\")\n",
    "\n",
    "    # Print a message indicating that the training process has finished.\n",
    "    print('Finished!')\n",
    "    # Read in the CSV file specified by 'path' and store it in '_results_csv'.\n",
    "    _results_csv = pd.read_csv(path)\n",
    "    # Return the '_results_csv' and 'lrs' variables.\n",
    "    return _results_csv, lrs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Code execution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def execute(dataset_name):\n",
    "    # Define path to save results\n",
    "    path = f\"{dataset_name}_{CONFIG['model_name']}_dataset.csv\"\n",
    "    # print(path)\n",
    "\n",
    "    # Define transformations for activation function\n",
    "    transform_train = {'alphas': alphas, 'fixed': False}\n",
    "    transform_tanh_1 = {'alphas': torch.tensor(0.9999), 'fixed': True}\n",
    "    transform_tanh_01 = {'alphas': torch.tensor(0.1), 'fixed': True}\n",
    "    transform_tanh_001 = {'alphas': torch.tensor(0.01), 'fixed': True}\n",
    "    transform_tanh_opt = {'alphas': alphas, 'fixed': True}\n",
    "\n",
    "    # Create a list of all transformations with names\n",
    "    all_transforms = [(\"Tanh [optimal]\", transform_tanh_opt), (\"Tanh [trainable]\", transform_train),\n",
    "                      (\"Tanh [0.01]\", transform_tanh_001), (\"Tanh [0.1]\", transform_tanh_01),\n",
    "                      (\"Tanh [1.0]\", transform_tanh_1)]\n",
    "\n",
    "    # Create a list to store dataset loaders for each transformation\n",
    "    instances = []\n",
    "\n",
    "    # Loop over all transformations and create a dataset loader for each one\n",
    "    for name, transform in all_transforms:\n",
    "        train_set = CustomDataset(X_train, y_train, mean, std)\n",
    "        test_set = CustomDataset(X_test, y_test, mean, std)\n",
    "        train_dl_stdz = torch.utils.data.DataLoader(train_set, batch_size=CONFIG['batch_size'], shuffle=True,\n",
    "                                                    pin_memory=CONFIG['pin_memory'], num_workers=CONFIG['n_workers'])\n",
    "        test_dl_stdz = torch.utils.data.DataLoader(test_set, batch_size=CONFIG['batch_size'], shuffle=True,\n",
    "                                                   pin_memory=CONFIG['pin_memory'], num_workers=CONFIG['n_workers'])\n",
    "        instances.append((name, train_dl_stdz, test_dl_stdz, transform))\n",
    "\n",
    "    # Run training.\n",
    "    _ = run_analysis(path, instances, alphas)\n",
    "    print(f\"done.\")\n",
    "    # Load the results from CSV and plot the scores for each epoch and transformation\n",
    "    results_csv = pd.read_csv(path)\n",
    "    sns.lineplot(data=results_csv, x=\"Epoch\", y=\"Score\", hue=\"Method\", style=\"Type\", markers=True)\n",
    "    print(f\"done.\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Execute, run code and print info.\n",
    "execute(CONFIG['dataset_name'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_alpha_history(path, title=\"\"):\n",
    "    # Read CSV file into a pandas dataframe.\n",
    "    df = pd.read_csv(path)\n",
    "    # Convert the dataframe to long form using the `melt()` function.\n",
    "    dfm = df.melt(id_vars=['Epoch', 'Type', 'Method'], value_vars=['Score'])\n",
    "    # Create a line plot using seaborn, with the x-axis using the first column of the dataframe, the y-axis\n",
    "    # using the \"value\" column created by the `melt()` function, and the lines colored based on the \"Method\" column.\n",
    "    g = sns.lineplot(data=dfm, x=df.columns[0], y=\"value\", hue='Method', style=\"Type\", markers=False)\n",
    "    # Set the title of the plot.\n",
    "    g.set_title(title)\n",
    "    # Plot the figure.\n",
    "    g.plot()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path = f\"synthetic_FeedForward_dataset.csv\"\n",
    "print_alpha_history(path, \"Accuracy: Synthetic data set\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
